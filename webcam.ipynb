{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.10.13\n",
            "<module 'mediapipe.python.solutions.hands' from '/Users/xuan035/miniforge3/envs/random/lib/python3.10/site-packages/mediapipe/python/solutions/hands.py'>\n"
          ]
        }
      ],
      "source": [
        "import mediapipe as mp\n",
        "print(mp.__version__)\n",
        "print(mp.solutions.hands)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "ohQQtXk12zDz",
        "outputId": "80959587-a2fb-4a92-dacd-71bab1a054da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1771819940.524504 3493352 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
            "/Users/xuan035/miniforge3/envs/random/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import mediapipe as mp\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from collections import deque, Counter\n",
        "import os\n",
        "\n",
        "# Updated webcam demo: supports left/right hand detection with per-hand gesture classification\n",
        "# Config\n",
        "MODEL_PATH = \"gesture_resnet18.pt\"   # same folder as this file\n",
        "CAMERA_INDEX = 0                     # change to 1 if needed\n",
        "SMOOTHING_WINDOW = 8\n",
        "\n",
        "CONF_THRESHOLD = 0.70\n",
        "UNKNOWN_LABEL = \"Unknown\"\n",
        "\n",
        "# Load model\n",
        "ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
        "CLASSES = ckpt[\"classes\"]\n",
        "IMG_SIZE = ckpt[\"img_size\"]\n",
        "\n",
        "model = resnet18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(CLASSES))\n",
        "model.load_state_dict(ckpt[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "# Transforms (must match training)\n",
        "tfm = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225],\n",
        "    ),\n",
        "])\n",
        "\n",
        "# MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(\n",
        "    static_image_mode=False,\n",
        "    max_num_hands=2, # changed from 1 to 2\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5,\n",
        ")\n",
        "\n",
        "# Utils\n",
        "def crop_from_landmarks(frame, hand_landmarks, pad=40):\n",
        "    h, w, _ = frame.shape\n",
        "    xs = [int(lm.x * w) for lm in hand_landmarks.landmark]\n",
        "    ys = [int(lm.y * h) for lm in hand_landmarks.landmark]\n",
        "\n",
        "    x1 = max(min(xs) - pad, 0)\n",
        "    x2 = min(max(xs) + pad, w)\n",
        "    y1 = max(min(ys) - pad, 0)\n",
        "    y2 = min(max(ys) + pad, h)\n",
        "\n",
        "    return frame[y1:y2, x1:x2], (x1, y1, x2, y2)\n",
        "\n",
        "# Runtime state\n",
        "pred_hist = {\n",
        "    \"Left\": deque(maxlen=SMOOTHING_WINDOW),\n",
        "    \"Right\": deque(maxlen=SMOOTHING_WINDOW),\n",
        "}\n",
        "\n",
        "cap = cv2.VideoCapture(CAMERA_INDEX)\n",
        "\n",
        "# Main loop\n",
        "while True:\n",
        "    ok, frame = cap.read()\n",
        "    if not ok:\n",
        "        break\n",
        "\n",
        "    # mirror view\n",
        "    frame = cv2.flip(frame, 1)\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    res = hands.process(rgb)\n",
        "\n",
        "    if res.multi_hand_landmarks and res.multi_handedness:\n",
        "        for i, hlm in enumerate(res.multi_hand_landmarks):\n",
        "\n",
        "            handedness = res.multi_handedness[i].classification[0].label  # Left / Right\n",
        "\n",
        "            # because frame is flipped\n",
        "            handedness = \"Left\" if handedness == \"Right\" else \"Right\"\n",
        "\n",
        "            crop, (x1, y1, x2, y2) = crop_from_landmarks(frame, hlm, pad=50)\n",
        "            if crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            pil = Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB))\n",
        "            x = tfm(pil).unsqueeze(0)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                probs = torch.softmax(logits, dim=1)[0]\n",
        "                idx = int(torch.argmax(probs))\n",
        "                conf = float(probs[idx])\n",
        "\n",
        "            if conf >= CONF_THRESHOLD:\n",
        "                pred_hist[handedness].append(idx)\n",
        "                smooth_idx = Counter(pred_hist[handedness]).most_common(1)[0][0]\n",
        "                label = CLASSES[smooth_idx]\n",
        "            else:\n",
        "                label = UNKNOWN_LABEL\n",
        "\n",
        "            # draw bounding box\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "            # draw label\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                f\"{handedness}: {label} ({conf:.2f})\",\n",
        "                (x1, y1 - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.8,\n",
        "                (255, 255, 255),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            \"No hand\",\n",
        "            (20, 40),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            1.1,\n",
        "            (255, 255, 255),\n",
        "            2,\n",
        "        )\n",
        "\n",
        "    cv2.imshow(\"Gesture Demo\", frame)\n",
        "    if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
